---
title: "Selecting and fitting a model"
author: "Sanja Miklin"
date: "1/14/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Question 1

For each part, indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

a) The sample size $n$ is extremely large, and the number of predictors $p$ is small.

b) The number of predictors $p$ is extremely large, and the number of observations $n$ is small.

c) The relationship between the predictors and response is highly non-linear.

d) The variance of the error terms $\sigma^2 = \text{Var}(\epsilon)$ is extremely high.


## Question 2: Bias-variance decomposition

a) Generate a graph of typical (squared) bias, variance, training error, test error, and Bayes (irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.

```{r}
library(tidyverse)
library(broom)


#set up the simulated data

set.seed(123)


f = function(x) {
  5.055901 - 0.1848551 * x + 0.00748706 * x^2 - 0.00005543478*x^3
}

get_sim_data = function(f, sample_size = 50) {
  x = runif(n = sample_size, min = 0, max = 100)
  y = f(x) + rnorm(n = sample_size, mean = 0, sd = 0.6)
  data.frame(x, y)
}


# simulate multiple samples and model for each to ge bias/variance
# predict y at a certain x 

n_sims <- 1000
x0 = 19.11
y0 = f(x0) + rnorm(n = n_sims, mean = 0, sd = 0.6)

#set a df of predictions with degrees of freedom as above
errors <-  tibble(df = 2:30) 

#simulation loop    
for (i in 1:n_sims) {
  
    # simulate data
  sim_data = get_sim_data(f, sample_size = 50)
  
    # create models based on the data
  errors <- errors %>%
        mutate(model = map(df, ~ lm(y ~ splines::ns(x, df= .x), data = sim_data)))

    # 
  errors[[i+2]] <- as.numeric(map(errors$model, predict, newdata = data.frame(x = x0)))
  
  
  names(errors)[i+2] <- as.character(i)
}


# calculate bias and variance, and test mse
# taking test mse as the average mse for x0

get_bias = function(estimate, truth) {
  (mean(estimate) - truth)^2
}

get_mse = function(estimate, truth) {
  mean((estimate - truth) ^ 2)
}

errors$bias <- apply(errors[3:(n_sims+2)], 1, get_bias, f(x0))
errors$variance <- apply(errors[3:(n_sims+2)], 1, var)
errors$test <- apply(errors[3:(n_sims+2)], 1, get_mse, y0)

# using the most recent simulated data and model calculate train mse

errors <- errors %>%
  mutate(
         pred = map(model, augment, data = sim_data),
         train = map_dbl(pred, ~ mean(.$.resid^2)),
         #pred_test = map(model, augment, newdata = sim_test),
         #test = map_dbl(pred_test, ~ mean((.$y - .$.fitted)^2)),
         )

# transform the data frame for plotting, adding bayes error
# variance of the irreducible error is the square of the sd of the error term

errors <- errors %>%
    select(df, bias, variance, train, test) %>%
    mutate (bayes = 0.6^2) %>%
    gather(key, value, -df) %>%
    mutate(key = str_to_title(key))
    

# plot the stuff!
    
errors %>%    
  ggplot(aes(df, value, color = key)) +
  geom_smooth(se = FALSE) +
  scale_x_log10() +
  labs(x = "Flexibility",
       y = NULL,
       color = NULL) +
  theme(legend.position = "right")
    

```

b) Explain why each of the five curves has the shape displayed in part (a).
**Bayes (irreducible) error** is a constant because it is a constant that has to do with the data (specifically $\epsilon$), not with the models that we are choosing. 
We see **squared bias** decreasing as the flexibility increases because more flexible methods will fit the model to the sample better. In contrast, **variance** increases with flexibilityâ€”the more flexible the model,the better it will fit a certain sample but because of overfitting, the flexible models will varry more with each different sample.
This tradeoff is also seen with the  training and testing MSE: the more flexible the model, the more it can minimize the **train MSE**. To a certain point, more flexible models will also decrease the **test MSE** because they are better models, but if too flexible, they overfit the sample data which will result in a larger test MSE (therefore the U-shape)
Test MSE also equals to the sum of the irreducible error, squared bias and variance, and it is clear from the plot how this results in the shape of the test ME line.

## Question 3: Bias-variance decomposition

For classification problems, the test error rate is minimized by a simple classifier that assigns each observation to the most likely class given its predictor values:

	\[\text{Pr}(Y=j|X=x_0)\]

where $x_0$ is the test observation and each possible class is represented by $J$. This is a **conditional probability** that $Y=j$, given the observed predictor vector $x_0$. This classifier is known as the **Bayes classifier**. If the response variable is binary (i.e. two classes), the Bayes classifier corresponds to predicting class one if $\text{Pr}(Y=j|X=x_0)>0.5$, and class two otherwise.

Figure 2.13 in [ISL] illustrates a simulated example defining the decision boundary for a hypothetical data set. Produce a graph illustrating this concept. Specifically, implement the following elements in your program:

a) Set your random number generator seed.
b) Simulate a dataset of $N=200$ with $X_1, X_2$ where $X_1, X_2$ are random uniform variables between $[-1,1]$. 
c) Calculate $Y=X_1+ X_1^2 + X_2 + X_2^2 + \epsilon$, where $\epsilon ~ N(\mu=0,\sigma^2=0.25)$.
d) $Y$is defined in terms of the log-odds of success on the domain $[-\infty, +\infty]$. Calculate the probability of success bounded between $[0,1]$.
e) Plot each of the data points on a graph and use color to indicate if the observation was a success or a failure.
f) Overlay the plot with Bayes decision boundary, calculated using $X_1, X_2$
g) Give your plot a meaningful title and axis labels.
h) The colored background grid is optional.

