---
title: "Selecting and fitting a model"
author: "Sanja Miklin"
date: "1/14/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

##Question 1

For each part, indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

**a) The sample size $n$ is extremely large, and the number of predictors $p$ is small.**

With a alrge sample size and a small number of predictors, a flexible method will probably peform better—because of the flexibility, it will fit the data better, and we would expect it to be a fairly accurate model ebcause of the large n and small p (overtiffing is unlikely to be a problem)

**b) The number of predictors $p$ is extremely large, and the number of observations $n$ is small.**

With a small n and a very large p, an inflexible method will problably be better, because overfitting with a flexible method would be a big issue. An inflexible method such as lasso regression will also force a decrease in p, making the model more interpretable.

**c) The relationship between the predictors and response is highly non-linear.**

With a highly non-linear relationship, we would expect the flexible method as more flexible methods allow for modeling of non-linear and increasingly complex relationships.

**d) The variance of the error terms $\sigma^2 = \text{Var}(\epsilon)$ is extremely high.**

In case of large variance of the error terms, we would expect an inflexible method to perform better, as a flexible method would capture too much noise in the model.


## Question 2: Bias-variance decomposition

 a) Generate a graph of typical (squared) bias, variance, training error, test error, and Bayes (irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.

```{r}
library(tidyverse)
library(broom)


#set up the simulated data

set.seed(123)


f = function(x) {
  5.055901 - 0.1848551 * x + 0.00748706 * x^2 - 0.00005543478*x^3
}

get_sim_data = function(f, sample_size = 50) {
  x = runif(n = sample_size, min = 0, max = 100)
  y = f(x) + rnorm(n = sample_size, mean = 0, sd = 0.6)
  data.frame(x, y)
}


# simulate multiple samples and model for each to ge bias/variance
# predict y at a certain x 

n_sims <- 1000
x0 = 19.11
y0 = f(x0) + rnorm(n = n_sims, mean = 0, sd = 0.6)

#set a df of predictions with degrees of freedom as above
errors <-  tibble(df = 2:30) 

#simulation loop    
for (i in 1:n_sims) {
  
    # simulate data
  sim_data = get_sim_data(f, sample_size = 50)
  
    # create models based on the data
  errors <- errors %>%
        mutate(model = map(df, ~ lm(y ~ splines::ns(x, df= .x), data = sim_data)))

    # 
  errors[[i+2]] <- as.numeric(map(errors$model, predict, newdata = data.frame(x = x0)))
  
  
  names(errors)[i+2] <- as.character(i)
}


# calculate bias and variance, and test mse
# taking test mse as the average mse for x0

get_bias = function(estimate, truth) {
  (mean(estimate) - truth)^2
}

get_mse = function(estimate, truth) {
  mean((estimate - truth) ^ 2)
}

errors$bias <- apply(errors[3:(n_sims+2)], 1, get_bias, f(x0))
errors$variance <- apply(errors[3:(n_sims+2)], 1, var)
errors$test <- apply(errors[3:(n_sims+2)], 1, get_mse, y0)

# using the most recent simulated data and model calculate train mse

errors <- errors %>%
  mutate(
         pred = map(model, augment, data = sim_data),
         train = map_dbl(pred, ~ mean(.$.resid^2)),
         #pred_test = map(model, augment, newdata = sim_test),
         #test = map_dbl(pred_test, ~ mean((.$y - .$.fitted)^2)),
         )

# transform the data frame for plotting, adding bayes error
# variance of the irreducible error is the square of the sd of the error term

errors <- errors %>%
    select(df, bias, variance, train, test) %>%
    mutate (bayes = 0.6^2) %>%
    gather(key, value, -df) %>%
    mutate(key = str_to_title(key))
    

# plot the stuff!
    
errors %>%    
  ggplot(aes(df, value, color = key)) +
  geom_smooth(se = FALSE) +
  scale_x_log10() +
  labs(x = "Flexibility",
       y = NULL,
       color = NULL) +
  theme(legend.position = "right")
    

```

 b) Explain why each of the five curves has the shape displayed in part (a).

**Bayes (irreducible) error** is a constant because it is a constant that has to do with the data (specifically $\epsilon$), not with the models that we are choosing. 
We see **squared bias** decreasing as the flexibility increases because more flexible methods will fit the model to the sample better. In contrast, **variance** increases with flexibility—the more flexible the model,the better it will fit a certain sample but because of overfitting, the flexible models will varry more with each different sample.
This tradeoff is also seen with the  training and testing MSE: the more flexible the model, the more it can minimize the **train MSE**. To a certain point, more flexible models will also decrease the **test MSE** because they are better models, but if too flexible, they overfit the sample data which will result in a larger test MSE (therefore the U-shape)
Test MSE also equals to the sum of the irreducible error, squared bias and variance, and it is clear from the plot how this results in the shape of the test ME line.

## Question 3

For classification problems, the test error rate is minimized by a simple classifier that assigns each observation to the most likely class given its predictor values:

$\text{Pr}(Y=j|X=x_0)$

where $x_0$ is the test observation and each possible class is represented by $J$. This is a **conditional probability** that $Y=j$, given the observed predictor vector $x_0$. This classifier is known as the **Bayes classifier**. If the response variable is binary (i.e. two classes), the Bayes classifier corresponds to predicting class one if $\text{Pr}(Y=j|X=x_0)>0.5$, and class two otherwise.

Figure 2.13 in [ISL] illustrates a simulated example defining the decision boundary for a hypothetical data set. Produce a graph illustrating this concept. Specifically, implement the following elements in your program:

a) Set your random number generator seed.
b) Simulate a dataset of $N=200$ with $X_1, X_2$ where $X_1, X_2$ are random uniform variables between $[-1,1]$. 
c) Calculate $Y=X_1+ X_1^2 + X_2 + X_2^2 + \epsilon$, where $\epsilon \sim N(\mu=0,\sigma^2=0.25)$.
d) $Y$ is defined in terms of the log-odds of success on the domain $[-\infty, +\infty]$. Calculate the probability of success bounded between $[0,1]$.
e) Plot each of the data points on a graph and use color to indicate if the observation was a success or a failure.
f) Overlay the plot with Bayes decision boundary, calculated using $X_1, X_2$
g) Give your plot a meaningful title and axis labels.
h) The colored background grid is optional.

```{r}
#set seed
set.seed(123)

#simulate dataset

dataset <- tibble(X1 = runif(n = 200, min = -1, max = 1), 
                  X2 = runif(n = 200, min = -1, max = 1)) 

# calculate Y 

dataset <- dataset %>%
    mutate(Y= X1 + X1^2+X2 + X2^2 + rnorm(n = 200, mean = 0, sd = sqrt(0.25)))

# calculate probability of success and assign success/fail
# using equation to convert log ods to probability
dataset <- dataset %>%
    mutate(probability = exp(Y)/(1+exp(Y)),
            outcome = ifelse(probability > 0.5, "Success", "Failure"))


# find values of X2 for each X1, for which probability of success is 0.5,
# that is Y=log(0.5/0/5) = log(1) = 0.

boundary <- tibble(X1 = seq(-1, 1, by = 0.001)) %>%
    mutate(X2_1 = (-1+sqrt(1-4*(X1+X1^2)))/2,
            X2_2 = (-1-sqrt(1-4*(X1+X1^2)))/2) 


# make a grid set

X1 <- seq(-1, 1, by = 0.02)
X2 <- seq(-1, 1, by = 0.02)
  
grid <- expand.grid(X1 = X1,X2 = X2) %>%
     mutate(Y= X1 + X1^2 + X2 + X2^2,
            probability = exp(Y)/(1+exp(Y)),
            outcome = ifelse(probability > 0.5, "Success", "Failure"))

# plot everything

ggplot() +
    geom_point(data = grid, aes(x=X1, y=X2, color = outcome), size = 0.01, alpha = 0.3)+
    geom_point(data = dataset, aes(x=X1, y=X2, color = outcome)) +
    geom_line(data = boundary,aes(x = X1, y=X2_1), linetype = "dashed") +
    geom_line(data = boundary,aes(x = X1, y=X2_2), linetype = "dashed") +
    scale_y_continuous(limits = c(-1, 1))+
    theme_bw() +
    labs(title = "Illustration of the Bayes clasifier",
        x = expression(paste(X[1])),
       y = expression(paste(X[2])),
       color = NULL)
    


```

